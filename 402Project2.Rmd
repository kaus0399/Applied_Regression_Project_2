---
title: "Project 2 Modeling & Interaction Effects"
author: "Group 6"
date: "11/30/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
players <- read.csv('clean_data.csv')
```

Below we create a column called `top5league` that is true when the player is in one of the top 5 leagues or false when the player is not in one of the top 5 leagues.

```{r}
library(dplyr)
players <- players %>% mutate(top5league=ifelse(league_name=='English Premier League' | league_name=='French Ligue 1'| league_name=='Spain Primera Division' | league_name=='German 1. Bundesliga' | league_name=='Italian Serie A', T,F))
```

```{r}
table(players$top5league)
```


```{r}
m1 <- aov(wage_eur ~ league_name_grouped, data = players)
summary(m1)
```

```{r}
tapply(players$wage_eur, players$league_name_grouped, mean, na.rm = TRUE)
```

We drop columns that are either meaningless to this study or that are directly related to our target variable, to make modeling easier later on.

```{r}
drops <- c('wage_eur', 'release_clause_eur', 'value_eur', 'sofifa_id', 'player_url', 'team_jersey_number', 'short_name', 'long_name', "ls", "st", "rs", "lw", "lf", "cf", "rf", "rw", "lam", "cam", "ram", "lm", "lcm", "cm", "rcm", "rm", "lwb", "ldm", "cdm", "rdm", "rwb", "lb", "lcb", "cb", "rcb", "rb", "dob", "body_type", "real_face")
players <- players[ , !(names(players) %in% drops)]
```

```{r}
head(players)
```
We create training and testing data sets.

```{r}
smp_size <- floor(0.75 * nrow(players))
set.seed(1)
train_ind <- sample(seq_len(nrow(players)), size = smp_size)

train <- players[train_ind, ]
test <- players[-train_ind, ]
```

We run random forest for feature importance.

```{r}
library(randomForest)
wage.rf <- randomForest(factor(wage_cat) ~ . - wage_eur_log, data = train, importance = TRUE, na.action = na.omit)
print(wage.rf)
```


```{r}
library(ggplot2)
library(dplyr)
# make dataframe from importance() output
feat_imp_df <- importance(wage.rf, type=2) %>% 
  data.frame() %>% 
  mutate(feature = row.names(.)) 

feat_imp_df <- feat_imp_df[order(feat_imp_df$MeanDecreaseGini, decreasing = TRUE),]
```

```{r}
feat_imp_df
```

We use the important features selected by random forest for our first model.

```{r}
library(nnet)
model1 <- multinom(wage_cat ~ overall + movement_reactions + mentality_composure + skill_ball_control + age + top5league + dribbling + potential + defending, data = train)
summary(model1, Wald.ratios = TRUE)
```

```{r}
library(caret)
pdata <- predict(model1, test)
confusionMatrix(pdata, reference = factor(test$wage_cat))
```

We replace `top5league` with `league_name_grouped` to see if the model performs better.

```{r}
model2 <- multinom(wage_cat ~ overall + movement_reactions + mentality_composure + skill_ball_control + age + league_name_grouped + dribbling + potential + defending, data = train)
summary(model2, Wald.ratios = TRUE)
```

```{r}
pdata <- predict(model2, test)
confusionMatrix(pdata, reference = factor(test$wage_cat))
```

We use the step() function.

```{r}
set.seed(1)
stepmodel <- step(model2)
```
```{r}
stepmodel
```

```{r}
pdata <- predict(stepmodel, test)
confusionMatrix(pdata, reference = factor(test$wage_cat))
```

The following models are models in which we manually choose the predictors, taking into account our knowledge of the sport.

```{r}
model3 <- multinom(wage_cat ~ overall + movement_reactions + mentality_composure + skill_ball_control + age + league_name_grouped, data = train)
summary(model3, Wald.ratios = TRUE)
```

```{r}
pdata <- predict(model3, test)
confusionMatrix(pdata, reference = factor(test$wage_cat))
```


```{r}
model4 <- multinom(wage_cat ~ overall + league_name_grouped + defending, data = train)
summary(model4, Wald.ratios = TRUE)
```

```{r}
pdata <- predict(model4, test)
confusionMatrix(pdata, reference = factor(test$wage_cat))
```


```{r}
model5 <- multinom(wage_cat ~ overall + defending + top5league, data = train)
summary(model5, Wald.ratios = TRUE)
```

```{r}
pdata <- predict(model5, test)
confusionMatrix(pdata, reference = factor(test$wage_cat))
```

Out of all the models so far, it seems like model 4 performs the best. Let's look into model 4 more.

```{r}
z <- summary(model4)$coefficients / summary(model4)$standard.errors
z
```

```{r}
p <- (1 - pnorm(abs(z), 0, 1))*2
p
```

```{r}
coef(model4)
```

```{r}
confint(model4)
```


```{r}
exp(coef(model4))
```

```{r}
exp(coef(model4)*10)
```

```{r}
exp(confint(model4))
```


```{r}
library(sjPlot)
plot_model(model4)
```

```{r}
lm <- lm(wage_eur_log ~ overall + league_name_grouped + defending, data=players)
summary(lm)
```

```{r}
library(car)
vif(lm)
```

```{r}
plot(lm)
```


```{r}
library(car)
Anova(model4)
```

```{r}
model6 <- multinom(wage_cat ~ league_name_grouped, data = train)
model7 <- multinom(wage_cat ~ overall, data = train)
model8 <- multinom(wage_cat ~ defending, data = train)
```

```{r}
library(effects)
plot(allEffects(model6), ask = FALSE)
plot(allEffects(model7), ask = FALSE)
plot(allEffects(model8), ask = FALSE)
```


```{r}
chisq.test(test$wage_cat,predict(model4, test))
```


Now let's delve into models with interaction effects.

```{r}
model9 <- multinom(wage_cat ~ league_name_grouped * overall + defending, data = train)
summary(model9)
plot(allEffects(model9))
```

```{r}
pdata <- predict(model9, test)
confusionMatrix(pdata, reference = factor(test$wage_cat))
```

```{r}
model10 <- multinom(wage_cat ~ league_name_grouped * defending + overall, data = train)
summary(model10)
plot(allEffects(model10))
```

```{r}
pdata <- predict(model10, test)
confusionMatrix(pdata, reference = factor(test$wage_cat))
```

```{r}
model11 <- multinom(wage_cat ~ league_name_grouped * overall, data = train)
summary(model11)
plot(allEffects(model11))
```


```{r}
pdata <- predict(model11, test)
confusionMatrix(pdata, reference = factor(test$wage_cat))
```

```{r}
model12 <- multinom(wage_cat ~ league_name_grouped * defending, data = train)
summary(model12)
plot(allEffects(model12))
```


```{r}
pdata <- predict(model12, test)
confusionMatrix(pdata, reference = factor(test$wage_cat))
```

It seems like model 10 does pretty well. Let's look into this model more.

```{r}
z <- summary(model10)$coefficients / summary(model10)$standard.errors
z
```

```{r}
p <- (1 - pnorm(abs(z), 0, 1))*2
p
```

```{r}
coef(model10)
```

```{r}
confint(model10)
```

```{r}
exp(coef(model10))
```

```{r}
exp(confint(model10))
```


```{r}
library(sjPlot)
plot_model(model10)
```

```{r}
print(paste("Pearson's X^2 =",round(sum(residuals(model10,type="pearson")^2),3)))
```


```{r}
Anova(model10)
```

```{r}
chisq.test(test$wage_cat,predict(model10, test))
```